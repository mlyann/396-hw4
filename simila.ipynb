{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlyang721/miniconda3/envs/fast/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "model_name = 'FacebookAI/roberta-base'\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load vocabulary from glove.6B.300d-vocabulary.txt\n",
    "def load_vocabulary(vocab_file):\n",
    "    with open(vocab_file, 'r') as f:\n",
    "        words = [line.strip() for line in f.readlines()]\n",
    "    return words\n",
    "\n",
    "# Tokenize vocabulary words\n",
    "def tokenize_words(words):\n",
    "    tokenized_words = {}\n",
    "    for word in words:\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        tokenized_words[word] = tokens\n",
    "    return tokenized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with some example words\n",
    "def test_examples(word_embeddings):\n",
    "    test_words = [\"cactus\", \"cake\", \"angry\", \"quickly\", \"between\", \"the\"]\n",
    "    for word in test_words:\n",
    "        try:\n",
    "            similar_words = most_similar(word, word_embeddings)\n",
    "            print(f\"Most similar words to '{word}': {similar_words}\")\n",
    "        except ValueError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from result.csv\n",
    "def load_embeddings(embedding_file):\n",
    "    embeddings = {}\n",
    "    with open(embedding_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            token = row[0]\n",
    "            embedding = np.fromstring(row[1].strip(\"[]\"), sep=',')\n",
    "            embeddings[token] = embedding\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute word embeddings by averaging token embeddings\n",
    "def compute_word_embeddings(tokenized_words, token_embeddings):\n",
    "    word_embeddings = {}\n",
    "    for word, tokens in tokenized_words.items():\n",
    "        token_vectors = [token_embeddings[token] for token in tokens if token in token_embeddings]\n",
    "        if token_vectors:\n",
    "            word_embeddings[word] = np.mean(token_vectors, axis=0)\n",
    "    return word_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar words based on cosine similarity\n",
    "def most_similar(word, word_embeddings, top_n=5):\n",
    "    if word not in word_embeddings:\n",
    "        return []\n",
    "    \n",
    "    word_vector = word_embeddings[word]\n",
    "    similarities = {}\n",
    "    for other_word, other_vector in word_embeddings.items():\n",
    "        if other_word != word:\n",
    "            similarities[other_word] = 1 - cosine(word_vector, other_vector)\n",
    "    # Sort by similarity score\n",
    "    return sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to cactus:\n",
      "creticus: 0.9831\n",
      "cantus: 0.9829\n",
      "cercocarpus: 0.9827\n",
      "carcasses: 0.9825\n",
      "crescentic: 0.9823\n",
      "Most similar words to cake:\n",
      "fruitcake: 1.0000\n",
      "cakebread: 0.9798\n",
      "cakewalk: 0.9794\n",
      "mooncake: 0.9783\n",
      "cakey: 0.9757\n",
      "Most similar words to angry:\n",
      "ryang: 1.0000\n",
      "ryanggang: 0.9932\n",
      "mlanghenry: 0.9872\n",
      "yungang: 0.9867\n",
      "yanchang: 0.9863\n",
      "Most similar words to quickly:\n",
      "quickies: 0.9838\n",
      "quick-fire: 0.9818\n",
      "quickness: 0.9811\n",
      "quickie: 0.9810\n",
      "quickplay: 0.9807\n",
      "Most similar words to between:\n",
      "inbetween: 0.9815\n",
      "betweenness: 0.9790\n",
      "inbetweeners: 0.9704\n",
      "in-between: 0.9681\n",
      "go-between: 0.9675\n",
      "Most similar words to the:\n",
      "theorem: 1.0000\n",
      "theocrats: 1.0000\n",
      "andthe: 0.9822\n",
      "thet: 0.9822\n",
      "bythe: 0.9821\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary and embeddings\n",
    "vocab_file = 'glove.6B.300d-vocabulary.txt'\n",
    "embedding_file = 'result.csv'\n",
    "\n",
    "words = load_vocabulary(vocab_file)\n",
    "tokenized_words = tokenize_words(words)\n",
    "token_embeddings = load_embeddings(embedding_file)\n",
    "word_embeddings = compute_word_embeddings(tokenized_words, token_embeddings)\n",
    "\n",
    "# Example usage\n",
    "examples = [\"cactus\", \"cake\", \"angry\",\"quickly\",\"between\",\"the\"]  # Replace with actual words\n",
    "for word in examples:\n",
    "    print(f\"Most similar words to {word}:\")\n",
    "    similar_words = most_similar(word, word_embeddings)\n",
    "    for similar, score in similar_words:\n",
    "        print(f\"{similar}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
